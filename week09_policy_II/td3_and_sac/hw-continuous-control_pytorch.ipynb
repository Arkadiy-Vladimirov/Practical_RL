{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T15:40:24.601903Z",
     "iopub.status.busy": "2024-05-04T15:40:24.601139Z",
     "iopub.status.idle": "2024-05-04T15:40:24.694687Z",
     "shell.execute_reply": "2024-05-04T15:40:24.693894Z",
     "shell.execute_reply.started": "2024-05-04T15:40:24.601855Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/setup_colab.sh -O- | bash\n",
    "\n",
    "    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/master/week09_policy_II/td3_and_sac/logger.py\n",
    "\n",
    "    !pip -q install gymnasium[mujoco]\n",
    "    !pip -q install tensorboardX\n",
    "\n",
    "    !touch .setup_complete\n",
    "\n",
    "# This code creates a virtual display to draw game images on.\n",
    "# It will have no effect if your machine has a monitor.\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n",
    "    !bash ../../xvfb start\n",
    "    os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T15:40:29.828276Z",
     "iopub.status.busy": "2024-05-04T15:40:29.827449Z",
     "iopub.status.idle": "2024-05-04T15:40:39.122138Z",
     "shell.execute_reply": "2024-05-04T15:40:39.121421Z",
     "shell.execute_reply.started": "2024-05-04T15:40:29.828237Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# in datasphere\n",
    "%pip -q install gymnasium[mujoco]\n",
    "%pip -q install tensorboardX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "\n",
    "In this notebook you will solve continuous control environment using either [Twin Delayed DDPG (TD3)](https://arxiv.org/pdf/1802.09477.pdf) or [Soft Actor-Critic (SAC)](https://arxiv.org/pdf/1801.01290.pdf). Both are off-policy algorithms that are current go-to algorithms for continuous control tasks.\n",
    "\n",
    "**Select one** of these two algorithms (TD3 or SAC) to implement. Both algorithms are extensions of basic [Deep Deterministic Policy Gradient (DDPG)](https://arxiv.org/abs/1509.02971) algorithm, and DDPG is kind of \"DQN with another neural net approximating greedy policy\", and all that differs is a set of stabilization tricks:\n",
    "* TD3 trains deterministic policy, while SAC uses *stochastic policy*. This means that for SAC you can solve exploration-exploitation trade-off by simple sampling from policy, while in TD3 you will have to add noise to your actions.\n",
    "* TD3 proposes to stabilize targets by adding a *clipped noise* to actions, which slightly prevents overestimation. In SAC, we formally switch to formalism of Maximum Entropy RL and add *entropy bonus* into our value function.\n",
    "\n",
    "Also both algorithms utilize a *twin trick*: train two critics and use pessimistic targets by taking minimum from two proposals. Standard trick with target networks is also necessary. We will go through all these tricks step-by-step.\n",
    "\n",
    "SAC is probably less clumsy scheme than TD3, but requires a bit more code to implement. More detailed description of algorithms can be found in Spinning Up documentation:\n",
    "* on [DDPG](https://spinningup.openai.com/en/latest/algorithms/ddpg.html)\n",
    "* on [TD3](https://spinningup.openai.com/en/latest/algorithms/td3.html)\n",
    "* on [SAC](https://spinningup.openai.com/en/latest/algorithms/sac.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T15:40:39.123876Z",
     "iopub.status.busy": "2024-05-04T15:40:39.123489Z",
     "iopub.status.idle": "2024-05-04T15:40:40.268224Z",
     "shell.execute_reply": "2024-05-04T15:40:40.267499Z",
     "shell.execute_reply.started": "2024-05-04T15:40:39.123839Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will create an instance of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:41:00.003174Z",
     "start_time": "2020-09-16T18:40:59.921640Z"
    },
    "execution": {
     "iopub.execute_input": "2024-05-04T15:40:40.269932Z",
     "iopub.status.busy": "2024-05-04T15:40:40.269520Z",
     "iopub.status.idle": "2024-05-04T15:40:40.537276Z",
     "shell.execute_reply": "2024-05-04T15:40:40.536671Z",
     "shell.execute_reply.started": "2024-05-04T15:40:40.269903Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kernel/lib/python3.10/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space:  Box(-inf, inf, (27,), float64) \n",
      "observations: [ 0.83114369  0.99284615  0.06177068  0.07004935  0.0743908   0.08116205\n",
      " -0.02185254 -0.01105408 -0.07682864 -0.08016159 -0.01568825  0.07353965\n",
      "  0.03087017 -0.07940711 -0.09943984  0.10287089  0.11755015  0.01726927\n",
      "  0.03348727 -0.11995557 -0.04173737 -0.13572624  0.22365714 -0.04446761\n",
      " -0.10630129 -0.05961107  0.11388347]\n",
      "action space:  Box(-1.0, 1.0, (8,), float32) \n",
      "action_sample:  [ 0.7024731   0.52976865 -0.591023   -0.02049428 -0.47039294  0.20231088\n",
      " -0.7916469  -0.11340988]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Ant-v4\", render_mode=\"rgb_array\")\n",
    "\n",
    "# we want to look inside\n",
    "env.reset()\n",
    "\n",
    "# examples of states and actions\n",
    "print(\"observation space: \", env.observation_space,\n",
    "      \"\\nobservations:\", env.reset()[0])\n",
    "print(\"action space: \", env.action_space,\n",
    "      \"\\naction_sample: \", env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T15:40:40.539001Z",
     "iopub.status.busy": "2024-05-04T15:40:40.538329Z",
     "iopub.status.idle": "2024-05-04T15:40:41.994941Z",
     "shell.execute_reply": "2024-05-04T15:40:41.994034Z",
     "shell.execute_reply.started": "2024-05-04T15:40:40.538960Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f2394cc5690>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(env.render())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run random policy and see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T15:40:41.996612Z",
     "iopub.status.busy": "2024-05-04T15:40:41.995977Z",
     "iopub.status.idle": "2024-05-04T15:40:42.007289Z",
     "shell.execute_reply": "2024-05-04T15:40:42.006537Z",
     "shell.execute_reply.started": "2024-05-04T15:40:41.996578Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RandomActor():\n",
    "    def get_action(self, states):\n",
    "        assert len(states.shape) == 1, \"can't work with batches\"\n",
    "        return env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T15:40:42.008686Z",
     "iopub.status.busy": "2024-05-04T15:40:42.008236Z",
     "iopub.status.idle": "2024-05-04T15:40:45.425830Z",
     "shell.execute_reply": "2024-05-04T15:40:45.425137Z",
     "shell.execute_reply.started": "2024-05-04T15:40:42.008656Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "done:  71\n",
      "done:  127\n",
      "done:  179\n",
      "done:  1179\n",
      "done:  1263\n",
      "done:  1362\n",
      "done:  1419\n",
      "done:  2419\n",
      "done:  2505\n",
      "done:  2564\n",
      "done:  3564\n",
      "done:  3697\n",
      "done:  3711\n",
      "done:  3744\n",
      "done:  3783\n",
      "done:  3839\n",
      "done:  4839\n",
      "done:  4933\n",
      "done:  5011\n",
      "done:  5029\n",
      "done:  5089\n",
      "done:  5129\n",
      "done:  5295\n",
      "done:  5327\n",
      "done:  5450\n",
      "done:  5470\n",
      "done:  5523\n",
      "done:  5533\n",
      "done:  6533\n",
      "done:  6664\n",
      "done:  6918\n",
      "done:  6954\n",
      "done:  6969\n",
      "done:  7010\n",
      "done:  7243\n",
      "done:  7279\n",
      "done:  7486\n",
      "done:  7500\n",
      "done:  7571\n",
      "done:  7671\n",
      "done:  7708\n",
      "done:  7764\n",
      "done:  7823\n",
      "done:  7855\n",
      "done:  8855\n",
      "done:  8887\n",
      "done:  9329\n"
     ]
    }
   ],
   "source": [
    "s, _ = env.reset()\n",
    "print(type(s))\n",
    "rewards_per_step = []\n",
    "actor = RandomActor()\n",
    "\n",
    "for i in range(10000):\n",
    "    a = actor.get_action(s)\n",
    "    s, r, terminated, truncated, _ = env.step(a)\n",
    "\n",
    "    rewards_per_step.append(r)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        s, _ = env.reset()\n",
    "        print(\"done: \", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, basically most episodes are 1000 steps long (then happens termination by time), though sometimes we are terminated earlier if simulation discovers some obvious reasons to think that we crashed our ant. Important thing about continuous control tasks like this is that we receive non-trivial signal at each step: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T15:40:45.427816Z",
     "iopub.status.busy": "2024-05-04T15:40:45.427152Z",
     "iopub.status.idle": "2024-05-04T15:40:45.451299Z",
     "shell.execute_reply": "2024-05-04T15:40:45.450675Z",
     "shell.execute_reply.started": "2024-05-04T15:40:45.427784Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.49083759726420606,\n",
       " -0.7608205164936059,\n",
       " -0.6445857311991219,\n",
       " -0.4706255163791999,\n",
       " 0.03823467412046089,\n",
       " -0.8858030025231516,\n",
       " -0.46433131412614426,\n",
       " 0.03057993806241832,\n",
       " -0.42078044323393105,\n",
       " 0.6142648954503338]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_per_step[100:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dense signal will guide our optimizations. It also partially explains why off-policy algorithms are more effective and sample-efficient than on-policy algorithms like PPO: 1-step targets are already quite informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T15:40:45.452909Z",
     "iopub.status.busy": "2024-05-04T15:40:45.452199Z",
     "iopub.status.idle": "2024-05-04T15:40:45.484600Z",
     "shell.execute_reply": "2024-05-04T15:40:45.483937Z",
     "shell.execute_reply.started": "2024-05-04T15:40:45.452877Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will add only one wrapper to our environment to simply write summaries, mainly, the total reward during an episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T15:40:45.850247Z",
     "iopub.status.busy": "2024-05-04T15:40:45.849787Z",
     "iopub.status.idle": "2024-05-04T15:40:52.279620Z",
     "shell.execute_reply": "2024-05-04T15:40:52.279002Z",
     "shell.execute_reply.started": "2024-05-04T15:40:45.850210Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from logger import TensorboardSummaries as Summaries\n",
    "\n",
    "env = gym.make(\"Ant-v4\", render_mode=\"rgb_array\")\n",
    "env = Summaries(env, \"MyFirstWalkingAnt\");\n",
    "\n",
    "state_dim = env.observation_space.shape[0]  # dimension of state space (27 numbers)\n",
    "action_dim = env.action_space.shape[0]      # dimension of action space (8 numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with *critic* model. On the one hand, it will function as an approximation of $Q^*(s, a)$, on the other hand it evaluates current actor $\\pi$ and can be viewed as $Q^{\\pi}(s, a)$. This critic will take both state $s$ and action $a$ as input and output a scalar value. Recommended architecture is 3-layered MLP.\n",
    "\n",
    "**Danger:** when models have a scalar output it is a good rule to squeeze it to avoid unexpected broadcasting, since [batch_size, 1] broadcasts with many tensor sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T15:40:52.281607Z",
     "iopub.status.busy": "2024-05-04T15:40:52.280835Z",
     "iopub.status.idle": "2024-05-04T15:40:52.358306Z",
     "shell.execute_reply": "2024-05-04T15:40:52.357687Z",
     "shell.execute_reply.started": "2024-05-04T15:40:52.281573Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()        \n",
    "\n",
    "        self._layers = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def get_qvalues(self, states, actions):\n",
    "        '''\n",
    "        input:\n",
    "            states - tensor, (batch_size x features)\n",
    "            actions - tensor, (batch_size x actions_dim)\n",
    "        output:\n",
    "            qvalues - tensor, critic estimation, (batch_size)\n",
    "        '''\n",
    "        qvalues = self._layers(torch.cat([states, actions], dim=-1)).squeeze()\n",
    "\n",
    "        assert len(qvalues.shape) == 1 and qvalues.shape[0] == states.shape[0]\n",
    "        \n",
    "        return qvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's define a policy, or an actor $\\pi$. Use architecture, similar to critic (3-layered MLP). The output depends on algorithm:\n",
    "\n",
    "For **TD3**, model *deterministic policy*. You should output `action_dim` numbers in range $[-1, 1]$. Unfortunately, deterministic policies lead to problems with stability and exploration, so we will need three \"modes\" of how this policy can be operating:\n",
    "* First one - greedy - is a simple feedforward pass through network that will be used to train the actor.\n",
    "* Second one - exploration mode - is when we need to add noise (e.g. Gaussian) to our actions to collect more diverse data. \n",
    "* Third mode - \"clipped noised\" - will be used when we will require a target for critic, where we need to somehow \"noise\" our actor output, but not too much, so we add *clipped noise* to our output:\n",
    "$$\\pi_{\\theta}(s) + \\varepsilon, \\quad \\varepsilon = \\operatorname{clip}(\\epsilon, -0.5, 0.5), \\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:41:06.246418Z",
     "start_time": "2020-09-16T18:41:05.841255Z"
    },
    "execution": {
     "iopub.execute_input": "2024-05-04T15:40:52.360141Z",
     "iopub.status.busy": "2024-05-04T15:40:52.359680Z",
     "iopub.status.idle": "2024-05-04T15:40:52.373089Z",
     "shell.execute_reply": "2024-05-04T15:40:52.372532Z",
     "shell.execute_reply.started": "2024-05-04T15:40:52.360107Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# template for TD3; template for SAC is below\n",
    "class TD3_Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()        \n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_action(self, states, std_noise=0.1):\n",
    "        '''\n",
    "        Used to collect data by interacting with environment,\n",
    "        so your have to add some noise to actions.\n",
    "        input:\n",
    "            states - numpy, (batch_size x features)\n",
    "        output:\n",
    "            actions - numpy, (batch_size x actions_dim)\n",
    "        '''\n",
    "        # no gradient computation is required here since we will use this only for interaction\n",
    "        with torch.no_grad():\n",
    "            # actions = <YOUR CODE>\n",
    "            raise NotImplementedError\n",
    "\n",
    "            assert isinstance(actions, (list,np.ndarray)), \"convert actions to numpy to send into env\"\n",
    "            assert actions.max() <= 1. and actions.min() >= -1, \"actions must be in the range [-1, 1]\"\n",
    "            return actions\n",
    "    \n",
    "    def get_best_action(self, states):\n",
    "        '''\n",
    "        Will be used to optimize actor. Requires differentiable w.r.t. parameters actions.\n",
    "        input:\n",
    "            states - PyTorch tensor, (batch_size x features)\n",
    "        output:\n",
    "            actions - PyTorch tensor, (batch_size x actions_dim)\n",
    "        '''\n",
    "        # actions = <YOUR CODE>\n",
    "        raise NotImplementedError\n",
    "    \n",
    "        assert actions.requires_grad, \"you must be able to compute gradients through actions\"\n",
    "        return actions\n",
    "    \n",
    "    def get_target_action(self, states, std_noise=0.2, clip_eta=0.5):\n",
    "        '''\n",
    "        Will be used to create target for critic optimization.\n",
    "        Returns actions with added \"clipped noise\".\n",
    "        input:\n",
    "            states - PyTorch tensor, (batch_size x features)\n",
    "        output:\n",
    "            actions - PyTorch tensor, (batch_size x actions_dim)\n",
    "        '''\n",
    "        # no gradient computation is required here since we will use this only for interaction\n",
    "        with torch.no_grad():\n",
    "            # actions = <YOUR CODE>\n",
    "            raise NotImplementedError\n",
    "        \n",
    "            # actions can fly out of [-1, 1] range after added noise\n",
    "            return actions.clamp(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **SAC**, model *gaussian policy*. This means policy distribution is going to be multivariate normal with diagonal covariance. The policy head will predict the mean and covariance, and it should be guaranteed that covariance is non-negative. **Important:** the way you model covariance strongly influences optimization procedure, so here are some options: let $f_{\\theta}$ be the output of covariance head, then:\n",
    "* use exponential function $\\sigma(s) = \\exp(f_{\\theta}(s))$\n",
    "* transform output to $[-1, 1]$ using `tanh`, then project output to some interval $[m, M]$, where $m = -20$, $M = 2$ and then use exponential function. This will guarantee the range of modeled covariance is adequate. So, the resulting formula is:\n",
    "$$\\sigma(s) = \\exp^{m + 0.5(M - m)(\\tanh(f_{\\theta}(s)) + 1)}$$\n",
    "* `softplus` operation $\\sigma(s) = \\log(1 + \\exp^{f_{\\theta}(s)})$ seems to work poorly here. o_O\n",
    "\n",
    "**Note**: `torch.distributions.Normal` already has everything you will need to work with such policy after you modeled mean and covariance, i.e. sampling via reparametrization trick (see `rsample` method) and compute log probability (see `log_prob` method).\n",
    "\n",
    "There is one more problem with gaussian distribution. We need to force our actions to be in $[-1, 1]$ bound. To achieve this, model unbounded gaussian $\\mathcal{N}(\\mu_{\\theta}(s), \\sigma_{\\theta}(s)^2I)$, where $\\mu$ can be arbitrary. Then every time you have samples $u$ from this gaussian policy, squash it using $\\operatorname{tanh}$ function to get a sample from $[-1, 1]$:\n",
    "$$u \\sim \\mathcal{N}(\\mu, \\sigma^2I)$$\n",
    "$$a = \\operatorname{tanh}(u)$$\n",
    "\n",
    "**Important:** after that you are required to use change of variable formula every time you compute likelihood (see appendix C in [paper on SAC](https://arxiv.org/pdf/1801.01290.pdf) for details):\n",
    "$$\\log p(a \\mid \\mu, \\sigma) = \\log p(u \\mid \\mu, \\sigma) - \\sum_{i = 1}^D \\log (1 - \\operatorname{tanh}^2(u_i)),$$\n",
    "where $D$ is `action_dim`. In practice, add something like 1e-6 inside logarithm to protect from computational instabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:41:06.246418Z",
     "start_time": "2020-09-16T18:41:05.841255Z"
    },
    "execution": {
     "iopub.execute_input": "2024-05-04T15:40:52.374549Z",
     "iopub.status.busy": "2024-05-04T15:40:52.374108Z",
     "iopub.status.idle": "2024-05-04T15:40:52.393629Z",
     "shell.execute_reply": "2024-05-04T15:40:52.393003Z",
     "shell.execute_reply.started": "2024-05-04T15:40:52.374520Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# template for SAC\n",
    "from torch.distributions import Normal\n",
    "\n",
    "class SAC_Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()        \n",
    "\n",
    "        self._std_lb = -20\n",
    "        self._std_ub = 2\n",
    "\n",
    "        self._backbone = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self._mean_head = nn.Sequential(\n",
    "            nn.Linear(256, action_dim),\n",
    "        )\n",
    "        self._std_head =  nn.Sequential(\n",
    "            nn.Linear(256, action_dim),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "    \n",
    "    def _action_mean(self, states):\n",
    "        return self._mean_head(self._backbone(states))\n",
    "\n",
    "    def _action_std(self, states):\n",
    "        return self._scale_std(self._std_head(self._backbone(states)))\n",
    "\n",
    "    def _scale_std(self, std):\n",
    "        return torch.exp(self._std_lb + 0.5 * (self._std_ub - self._std_lb) * (std + 1))\n",
    "\n",
    "    def _action_dist(self, states):\n",
    "        mean = self._action_mean(states)\n",
    "        std = self._action_std(states)\n",
    "        return Normal(mean, std)\n",
    "\n",
    "    @staticmethod\n",
    "    def _change_of_variables_log_prob_additive_correction(sample):\n",
    "        return torch.negative(torch.log(1 - torch.square(torch.tanh(sample)) + 1e-6))\n",
    "\n",
    "    def apply(self, states):\n",
    "        '''\n",
    "        For given batch of states samples actions and also returns its log prob.\n",
    "        input:\n",
    "            states - PyTorch tensor, (batch_size x features)\n",
    "        output:\n",
    "            actions - PyTorch tensor, (batch_size x action_dim)\n",
    "            log_prob - PyTorch tensor, (batch_size)\n",
    "        '''\n",
    "\n",
    "        dist = self._action_dist(states)\n",
    "        u = dist.rsample()\n",
    "        actions = torch.tanh(u)\n",
    "        log_prob = torch.sum(dist.log_prob(u) + self._change_of_variables_log_prob_additive_correction(u), dim=-1)\n",
    "        \n",
    "        return actions, log_prob  \n",
    "\n",
    "    def get_action(self, states):\n",
    "        '''\n",
    "        Used to interact with environment by sampling actions from policy\n",
    "        input:\n",
    "            states - numpy, (batch_size x features)\n",
    "        output:\n",
    "            actions - numpy, (batch_size x actions_dim)\n",
    "        '''\n",
    "        # no gradient computation is required here since we will use this only for interaction\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # hint: you can use `apply` method here\n",
    "            actions = self.apply(torch.from_numpy(states).float().to(DEVICE))[0].numpy(force=True)\n",
    "            \n",
    "            assert isinstance(actions, (list,np.ndarray)), \"convert actions to numpy to send into env\"\n",
    "            assert actions.max() <= 1. and actions.min() >= -1, \"actions must be in the range [-1, 1]\"\n",
    "            return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReplayBuffer\n",
    "\n",
    "The same as in DQN. You can copy code from your DQN assignment, just check that it works fine with continuous actions (probably it is). \n",
    "\n",
    "Let's recall the interface:\n",
    "* `exp_replay.add(obs, act, rw, next_obs, done)` - saves (s,a,r,s',done) tuple into the buffer\n",
    "* `exp_replay.sample(batch_size)` - returns observations, actions, rewards, next_observations and is_done for `batch_size` random samples.\n",
    "* `len(exp_replay)` - returns number of elements stored in replay buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T15:40:52.396394Z",
     "iopub.status.busy": "2024-05-04T15:40:52.395420Z",
     "iopub.status.idle": "2024-05-04T15:40:52.417804Z",
     "shell.execute_reply": "2024-05-04T15:40:52.417263Z",
     "shell.execute_reply.started": "2024-05-04T15:40:52.396359Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, size):\n",
    "        \"\"\"\n",
    "        Create Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "\n",
    "        Note: for this assignment you can pick any data structure you want.\n",
    "              If you want to keep it simple, you can store a list of tuples of (s, a, r, s') in self._storage\n",
    "              However you may find out there are faster and/or more memory-efficient ways to do so.\n",
    "        \"\"\"\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "        self._next_idx = 0\n",
    "\n",
    "        # OPTIONAL: YOUR CODE\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
    "        '''\n",
    "        Make sure, _storage will not exceed _maxsize. \n",
    "        Make sure, FIFO rule is being followed: the oldest examples has to be removed earlier\n",
    "        '''        \n",
    "        data = (obs_t, action, reward, obs_tp1, done)\n",
    "        \n",
    "        if self._next_idx >= len(self._storage):\n",
    "            self._storage.append(data)\n",
    "        else:\n",
    "            self._storage[self._next_idx] = data\n",
    "        self._next_idx = (self._next_idx + 1) % self._maxsize\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            batch of observations\n",
    "        act_batch: np.array\n",
    "            batch of actions executed given obs_batch\n",
    "        rew_batch: np.array\n",
    "            rewards received as results of executing act_batch\n",
    "        next_obs_batch: np.array\n",
    "            next set of observations seen after executing act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        \"\"\"\n",
    "\n",
    "        idxes = np.random.choice(len(self._storage), size=batch_size)\n",
    "            # randomly generate batch_size integers\n",
    "            # to be used as indexes of samples\n",
    "            \n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], [] \n",
    "        for idx in idxes:\n",
    "            st, act, rew, next_st, don = self._storage[idx]\n",
    "            states.append(np.array(st, copy=False))\n",
    "            actions.append(np.array(act, copy=False))\n",
    "            rewards.append(rew)\n",
    "            next_states.append(np.array(next_st, copy=False))\n",
    "            dones.append(don)\n",
    "            # collect <s,a,r,s',done> for each index\n",
    "        \n",
    "        return (\n",
    "            np.array(states),\n",
    "            np.array(actions),\n",
    "            np.array(rewards),\n",
    "            np.array(next_states),\n",
    "            np.array(dones)\n",
    "        )\n",
    "            # <states>, <actions>, <rewards>, <next_states>, <is_done>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T15:40:52.419130Z",
     "iopub.status.busy": "2024-05-04T15:40:52.418430Z",
     "iopub.status.idle": "2024-05-04T15:40:52.433926Z",
     "shell.execute_reply": "2024-05-04T15:40:52.433293Z",
     "shell.execute_reply.started": "2024-05-04T15:40:52.419098Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_replay = ReplayBuffer(10)\n",
    "\n",
    "for _ in range(30):\n",
    "    exp_replay.add(env.reset()[0], env.action_space.sample(),\n",
    "                   1.0, env.reset()[0], done=False)\n",
    "\n",
    "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(\n",
    "    5)\n",
    "\n",
    "assert len(exp_replay) == 10, \"experience replay size should be 10 because that's what maximum capacity is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T15:40:52.435545Z",
     "iopub.status.busy": "2024-05-04T15:40:52.434731Z",
     "iopub.status.idle": "2024-05-04T15:40:52.445971Z",
     "shell.execute_reply": "2024-05-04T15:40:52.445348Z",
     "shell.execute_reply.started": "2024-05-04T15:40:52.435514Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def play_and_record(initial_state, agent, env, exp_replay, n_steps=1):\n",
    "    \"\"\"\n",
    "    Play the game for exactly n steps, record every (s,a,r,s', done) to replay buffer. \n",
    "    Whenever game ends, add record with done=True and reset the game.\n",
    "    It is guaranteed that env has done=False when passed to this function.\n",
    "\n",
    "    :returns: return sum of rewards over time and the state in which the env stays\n",
    "    \"\"\"\n",
    "    s = initial_state\n",
    "    #print(type(s))\n",
    "    sum_rewards = 0\n",
    "\n",
    "    # Play the game for n_steps as per instructions above\n",
    "    for t in range(n_steps):\n",
    "        #print(type(s))\n",
    "        # select action using policy with exploration\n",
    "        a = agent.get_action(s)\n",
    "        \n",
    "        ns, r, terminated, truncated, _ = env.step(a)\n",
    "\n",
    "        done = terminated or truncated\n",
    "        # exp_replay.add(s, a, r, ns, terminated) why?\n",
    "        exp_replay.add(s, a, r, ns, done)\n",
    "        \n",
    "        s = env.reset()[0] if done else ns\n",
    "        \n",
    "        sum_rewards += r        \n",
    "\n",
    "    return sum_rewards, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T15:40:52.448392Z",
     "iopub.status.busy": "2024-05-04T15:40:52.447602Z",
     "iopub.status.idle": "2024-05-04T15:40:53.477843Z",
     "shell.execute_reply": "2024-05-04T15:40:53.477150Z",
     "shell.execute_reply.started": "2024-05-04T15:40:52.448346Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "#testing your code.\n",
    "exp_replay = ReplayBuffer(2000)\n",
    "actor = SAC_Actor(state_dim, action_dim).to(DEVICE)\n",
    "\n",
    "state, _ = env.reset()\n",
    "play_and_record(state, actor, env, exp_replay, n_steps=1000)\n",
    "\n",
    "# if you're using your own experience replay buffer, some of those tests may need correction.\n",
    "# just make sure you know what your code does\n",
    "assert len(exp_replay) == 1000, \"play_and_record should have added exactly 1000 steps, \"\\\n",
    "                                 \"but instead added %i\" % len(exp_replay)\n",
    "is_dones = list(zip(*exp_replay._storage))[-1]\n",
    "\n",
    "for _ in range(100):\n",
    "    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(\n",
    "        10)\n",
    "    assert obs_batch.shape == next_obs_batch.shape == (10,) + (state_dim,)\n",
    "    assert act_batch.shape == (\n",
    "        10, action_dim), \"actions batch should have shape (10, 8) but is instead %s\" % str(act_batch.shape)\n",
    "    assert reward_batch.shape == (\n",
    "        10,), \"rewards batch should have shape (10,) but is instead %s\" % str(reward_batch.shape)\n",
    "    assert is_done_batch.shape == (\n",
    "        10,), \"is_done batch should have shape (10,) but is instead %s\" % str(is_done_batch.shape)\n",
    "    assert [int(i) in (0, 1)\n",
    "            for i in is_dones], \"is_done should be strictly True or False\"\n",
    "\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Let's start initializing our algorithm. Here is our hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T15:40:53.479475Z",
     "iopub.status.busy": "2024-05-04T15:40:53.479026Z",
     "iopub.status.idle": "2024-05-04T15:40:53.499233Z",
     "shell.execute_reply": "2024-05-04T15:40:53.498672Z",
     "shell.execute_reply.started": "2024-05-04T15:40:53.479441Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gamma=0.99                    # discount factor\n",
    "max_buffer_size = 10**5       # size of experience replay\n",
    "start_timesteps = 5000        # size of experience replay when start training\n",
    "timesteps_per_epoch=1         # steps in environment per step of network updates\n",
    "batch_size=128                # batch size for all optimizations\n",
    "max_grad_norm=10              # max grad norm for all optimizations\n",
    "tau=0.005                     # speed of updating target networks\n",
    "policy_update_freq=1          # frequency of actor update; vanilla choice is 2 for TD3 or 1 for SAC\n",
    "alpha=0.1                     # temperature for SAC\n",
    "\n",
    "# iterations passed\n",
    "n_iterations = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our experience replay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T15:40:53.500823Z",
     "iopub.status.busy": "2024-05-04T15:40:53.500111Z",
     "iopub.status.idle": "2024-05-04T15:40:53.515495Z",
     "shell.execute_reply": "2024-05-04T15:40:53.514631Z",
     "shell.execute_reply.started": "2024-05-04T15:40:53.500791Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# experience replay\n",
    "exp_replay = ReplayBuffer(max_buffer_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our models: *two* critics and one actor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T15:40:53.518172Z",
     "iopub.status.busy": "2024-05-04T15:40:53.517200Z",
     "iopub.status.idle": "2024-05-04T15:40:53.532857Z",
     "shell.execute_reply": "2024-05-04T15:40:53.532317Z",
     "shell.execute_reply.started": "2024-05-04T15:40:53.518139Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# models to train\n",
    "actor = SAC_Actor(state_dim, action_dim).to(DEVICE)\n",
    "critic1 = Critic(state_dim, action_dim).to(DEVICE)\n",
    "critic2 = Critic(state_dim, action_dim).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stabilize training, we will require **target networks** - slow updating copies of our models. In **TD3**, both critics and actor have their copies, in **SAC** it is assumed that only critics require target copies while actor is always used fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T15:40:54.280215Z",
     "iopub.status.busy": "2024-05-04T15:40:54.279418Z",
     "iopub.status.idle": "2024-05-04T15:40:54.297227Z",
     "shell.execute_reply": "2024-05-04T15:40:54.296674Z",
     "shell.execute_reply.started": "2024-05-04T15:40:54.280174Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target networks: slow-updated copies of actor and two critics\n",
    "target_critic1 = Critic(state_dim, action_dim).to(DEVICE)\n",
    "target_critic2 = Critic(state_dim, action_dim).to(DEVICE)\n",
    "#target_actor = TD3_Actor(state_dim, action_dim).to(DEVICE)  # comment this line if you chose SAC\n",
    "\n",
    "# initialize them as copies of original models\n",
    "target_critic1.load_state_dict(critic1.state_dict())\n",
    "target_critic2.load_state_dict(critic2.state_dict())\n",
    "#target_actor.load_state_dict(actor.state_dict())            # comment this line if you chose SAC "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In continuous control, target networks are usually updated using exponential smoothing:\n",
    "$$\\theta^{-} \\leftarrow \\tau \\theta + (1 - \\tau) \\theta^{-},$$\n",
    "where $\\theta^{-}$ are target network weights, $\\theta$ - fresh parameters, $\\tau$ - hyperparameter. This util function will do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T15:40:55.304368Z",
     "iopub.status.busy": "2024-05-04T15:40:55.303620Z",
     "iopub.status.idle": "2024-05-04T15:40:55.314662Z",
     "shell.execute_reply": "2024-05-04T15:40:55.313951Z",
     "shell.execute_reply.started": "2024-05-04T15:40:55.304328Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def update_target_networks(model, target_model):\n",
    "    for param, target_param in zip(model.parameters(), target_model.parameters()):\n",
    "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will have three optimization procedures to train our three models, so let's welcome our three Adams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T15:40:56.082612Z",
     "iopub.status.busy": "2024-05-04T15:40:56.081789Z",
     "iopub.status.idle": "2024-05-04T15:40:56.094451Z",
     "shell.execute_reply": "2024-05-04T15:40:56.093787Z",
     "shell.execute_reply.started": "2024-05-04T15:40:56.082544Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# optimizers: for every model we have\n",
    "opt_actor = torch.optim.Adam(actor.parameters(), lr=3e-4)\n",
    "opt_critic1 = torch.optim.Adam(critic1.parameters(), lr=3e-4)\n",
    "opt_critic2 = torch.optim.Adam(critic2.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T15:40:56.561058Z",
     "iopub.status.busy": "2024-05-04T15:40:56.560191Z",
     "iopub.status.idle": "2024-05-04T15:40:56.571188Z",
     "shell.execute_reply": "2024-05-04T15:40:56.570641Z",
     "shell.execute_reply.started": "2024-05-04T15:40:56.561017Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# just to avoid writing this code three times\n",
    "def optimize(name, model, optimizer, loss):\n",
    "    '''\n",
    "    Makes one step of SGD optimization, clips norm with max_grad_norm and \n",
    "    logs everything into tensorboard\n",
    "    '''\n",
    "    loss = loss.mean()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "    optimizer.step()\n",
    "\n",
    "    # logging\n",
    "    env.writer.add_scalar(name, loss.item(), n_iterations)\n",
    "    env.writer.add_scalar(name + \"_grad_norm\", grad_norm.item(), n_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic target computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's discuss our losses for critic and actor.\n",
    "\n",
    "To train both critics we would like to minimize MSE using 1-step targets: for one sampled transition $(s, a, r, s')$ it should look something like this:\n",
    "$$y(s, a) = r + \\gamma V(s').$$\n",
    "\n",
    "How do we evaluate next state and compute $V(s')$? Well, technically Monte-Carlo estimation looks simple:\n",
    "$$V(s') \\approx Q(s', a')$$\n",
    "where (important!) $a'$ is a sample from our current policy $\\pi(a' \\mid s')$.\n",
    "\n",
    "But our actor $\\pi$ will be actually trained to search for actions $a'$ where our critic gives big estimates, and this straightforward approach leads to serious overesimation issues. We require some hacks. First, we will use target networks for $Q$ (and **TD3** also uses target network for $\\pi$). Second, we will use *two* critics and take minimum across their estimations:\n",
    "$$V(s') = \\min_{i = 1,2} Q^{-}_i(s', a'),$$\n",
    "where $a'$ is sampled from target policy $\\pi^{-}(a' \\mid s')$ in **TD3** and from fresh policy $\\pi(a' \\mid s')$ in **SAC**.\n",
    "\n",
    "###### And the last but not the least:\n",
    "* in **TD3** to compute $a'$ use *mode with clipped noise* that will prevent our policy from exploiting narrow peaks in our critic approximation;\n",
    "* in **SAC** add (estimation of) entropy bonus in next state $s'$:\n",
    "$$V(s') = \\min_{i = 1,2} Q^{-}_i(s', a') - \\alpha \\log \\pi (a' \\mid s')$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T15:40:57.900431Z",
     "iopub.status.busy": "2024-05-04T15:40:57.899709Z",
     "iopub.status.idle": "2024-05-04T15:40:57.915130Z",
     "shell.execute_reply": "2024-05-04T15:40:57.914505Z",
     "shell.execute_reply.started": "2024-05-04T15:40:57.900397Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_critic_target(rewards, next_states, is_done):\n",
    "    '''\n",
    "    Important: use target networks for this method! Do not use \"fresh\" models except fresh policy in SAC!\n",
    "    input:\n",
    "        rewards - PyTorch tensor, (batch_size)\n",
    "        next_states - PyTorch tensor, (batch_size x features)\n",
    "        is_done - PyTorch tensor, (batch_size)\n",
    "    output:\n",
    "        critic target - PyTorch tensor, (batch_size)\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        next_actions, next_log_probs = actor.apply(next_states)\n",
    "    \n",
    "        q1 = target_critic1.get_qvalues(next_states, next_actions)\n",
    "        q2 = target_critic2.get_qvalues(next_states, next_actions)\n",
    "        \n",
    "        v = torch.minimum(q1, q2) - alpha * next_log_probs\n",
    "\n",
    "        critic_target = rewards + torch.logical_not(is_done) * gamma * v \n",
    "    \n",
    "    assert not critic_target.requires_grad, \"target must not require grad.\"\n",
    "    assert len(critic_target.shape) == 1, \"dangerous extra dimension in target?\"\n",
    "\n",
    "    return critic_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train actor we want simply to maximize\n",
    "$$\\mathbb{E}_{a \\sim \\pi(a \\mid s)} Q(s, a) \\to \\max_{\\pi}$$\n",
    "\n",
    "* in **TD3**, because of deterministic policy, the expectation reduces:\n",
    "$$Q(s, \\pi(s)) \\to \\max_{\\pi}$$\n",
    "* in **SAC**, use reparametrization trick to compute gradients and also do not forget to add entropy regularizer to motivate policy to be as stochastic as possible:\n",
    "$$\\mathbb{E}_{a \\sim \\pi(a \\mid s)} Q(s, a) - \\alpha \\log \\pi(a \\mid s) \\to \\max_{\\pi}$$\n",
    "\n",
    "**Note:** We will use (fresh) critic1 here as Q-functon to \"exploit\". You can also use both critics and again take minimum across their estimations (this is done in original implementation of **SAC** and not done in **TD3**), but this seems to be not of high importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T15:40:59.264437Z",
     "iopub.status.busy": "2024-05-04T15:40:59.263744Z",
     "iopub.status.idle": "2024-05-04T15:40:59.279951Z",
     "shell.execute_reply": "2024-05-04T15:40:59.279382Z",
     "shell.execute_reply.started": "2024-05-04T15:40:59.264395Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_actor_loss(states):\n",
    "    '''\n",
    "    Returns actor loss on batch of states\n",
    "    input:\n",
    "        states - PyTorch tensor, (batch_size x features)\n",
    "    output:\n",
    "        actor loss - PyTorch tensor, (batch_size)\n",
    "    '''\n",
    "    # make sure you have gradients w.r.t. actor parameters\n",
    "    actions, log_probs = actor.apply(states)\n",
    "    q_values = torch.minimum(critic1.get_qvalues(states, actions), critic2.get_qvalues(states, actions))\n",
    "    \n",
    "    assert actions.requires_grad, \"actions must be differentiable with respect to policy parameters\"\n",
    "    assert log_probs.requires_grad, \"log_probs must be differentiable with respect to policy parameters\"\n",
    "\n",
    "    # compute actor loss\n",
    "    actor_loss = torch.negative(q_values - alpha * log_probs) \n",
    "    return actor_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally combining all together and launching our algorithm. Your goal is to reach at least 1000 average reward during evaluation after training in this ant environment (*since this is a new hometask, this threshold might be updated, so at least just see if your ant learned to walk in the rendered simulation*).\n",
    "\n",
    "* rewards should rise more or less steadily in this environment. There can be some drops due to instabilities of algorithm, but it should eventually start rising after 100K-200K iterations. If no progress in reward is observed after these first 100K-200K iterations, there is a bug.\n",
    "* gradient norm appears to be quite big for this task, it is ok if it reaches 100-200 (we handled it with clip_grad_norm). Consider everything exploded if it starts growing exponentially, then there is a bug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T15:41:00.949686Z",
     "iopub.status.busy": "2024-05-04T15:41:00.949013Z",
     "iopub.status.idle": "2024-05-04T15:41:01.004094Z",
     "shell.execute_reply": "2024-05-04T15:41:01.003468Z",
     "shell.execute_reply.started": "2024-05-04T15:41:00.949642Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 73\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T15:41:02.153517Z",
     "iopub.status.busy": "2024-05-04T15:41:02.152821Z",
     "iopub.status.idle": "2024-05-04T17:13:18.484211Z",
     "shell.execute_reply": "2024-05-04T17:13:18.483540Z",
     "shell.execute_reply.started": "2024-05-04T15:41:02.153485Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ecc744a17b4eac9bdf5af7e9e52f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import trange\n",
    "\n",
    "interaction_state, _ = env.reset(seed=seed)\n",
    "random_actor = RandomActor()\n",
    "\n",
    "n_iters = 500000\n",
    "for n_iterations in trange(0, n_iters, timesteps_per_epoch):\n",
    "    # if experience replay is small yet, no training happens\n",
    "    # we also collect data using random policy to collect more diverse starting data\n",
    "    if len(exp_replay) < start_timesteps:\n",
    "        _, interaction_state = play_and_record(interaction_state, random_actor, env, exp_replay, timesteps_per_epoch)\n",
    "        continue\n",
    "        \n",
    "    # perform a step in environment and store it in experience replay\n",
    "    _, interaction_state = play_and_record(interaction_state, actor, env, exp_replay, timesteps_per_epoch)\n",
    "        \n",
    "    # sample a batch from experience replay\n",
    "    states, actions, rewards, next_states, is_done = exp_replay.sample(batch_size)\n",
    "    \n",
    "    # move everything to PyTorch tensors\n",
    "    states = torch.tensor(states, device=DEVICE, dtype=torch.float)\n",
    "    actions = torch.tensor(actions, device=DEVICE, dtype=torch.float)\n",
    "    rewards = torch.tensor(rewards, device=DEVICE, dtype=torch.float)\n",
    "    next_states = torch.tensor(next_states, device=DEVICE, dtype=torch.float)\n",
    "    is_done = torch.tensor(\n",
    "        is_done.astype('float32'),\n",
    "        device=DEVICE,\n",
    "        dtype=torch.float\n",
    "    )\n",
    "    \n",
    "    # losses\n",
    "    critic_target = compute_critic_target(rewards, next_states, is_done)\n",
    "\n",
    "    critic1_loss = torch.square(critic1.get_qvalues(states, actions) - critic_target)\n",
    "    optimize(\"critic1\", critic1, opt_critic1, critic1_loss)\n",
    "\n",
    "    critic2_loss = torch.square(critic2.get_qvalues(states, actions) - critic_target)\n",
    "    optimize(\"critic2\", critic2, opt_critic2, critic2_loss)\n",
    "\n",
    "    # actor update is less frequent in TD3\n",
    "    if n_iterations % policy_update_freq == 0:\n",
    "        actor_loss = compute_actor_loss(states)\n",
    "        optimize(\"actor\", actor, opt_actor, actor_loss)\n",
    "\n",
    "        # update target networks\n",
    "        update_target_networks(critic1, target_critic1)\n",
    "        update_target_networks(critic2, target_critic2)\n",
    "        #update_target_networks(actor, target_actor)                     # comment this line if you chose SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T17:36:50.831608Z",
     "iopub.status.busy": "2024-05-04T17:36:50.830898Z",
     "iopub.status.idle": "2024-05-04T17:36:50.858837Z",
     "shell.execute_reply": "2024-05-04T17:36:50.858216Z",
     "shell.execute_reply.started": "2024-05-04T17:36:50.831569Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kernel/lib/python3.10/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "torch.save(actor.state_dict(), \"./models/SAC-actor\")\n",
    "torch.save(critic1.state_dict(), \"./models/SAC-critic1\")\n",
    "torch.save(critic2.state_dict(), \"./models/SAC-critic2\")\n",
    "torch.save(target_critic1.state_dict(), \"./models/SAC-target_critic1\")\n",
    "torch.save(target_critic2.state_dict(), \"./models/SAC-target_critic2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor = SAC_Actor(state_dim, action_dim).to(DEVICE)\n",
    "actor.load_state_dict(torch.load(\"./models/SAC-actor\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:41:47.560269Z",
     "start_time": "2020-09-16T18:41:47.546277Z"
    },
    "execution": {
     "iopub.execute_input": "2024-05-04T17:30:30.657486Z",
     "iopub.status.busy": "2024-05-04T17:30:30.656776Z",
     "iopub.status.idle": "2024-05-04T17:30:30.685096Z",
     "shell.execute_reply": "2024-05-04T17:30:30.684414Z",
     "shell.execute_reply.started": "2024-05-04T17:30:30.657439Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(env, actor, n_games=1, t_max=1000):\n",
    "    '''\n",
    "    Plays n_games and returns rewards and rendered games\n",
    "    '''\n",
    "    rewards = []\n",
    "\n",
    "    for _ in range(n_games):\n",
    "        s, _ = env.reset()\n",
    "\n",
    "        R = 0\n",
    "        for _ in range(t_max):\n",
    "            # select action for final evaluation of your policy\n",
    "            action = actor.get_action(s)\n",
    "\n",
    "            assert (action.max() <= 1).all() and  (action.min() >= -1).all()\n",
    "\n",
    "            s, r, terminated, truncated, _ = env.step(action)\n",
    "\n",
    "            R += r\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        rewards.append(R)\n",
    "    return np.array(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:38:45.130920Z",
     "start_time": "2020-09-16T18:38:13.090472Z"
    },
    "execution": {
     "iopub.execute_input": "2024-05-04T17:30:35.105068Z",
     "iopub.status.busy": "2024-05-04T17:30:35.104276Z",
     "iopub.status.idle": "2024-05-04T17:30:47.724801Z",
     "shell.execute_reply": "2024-05-04T17:30:47.724145Z",
     "shell.execute_reply.started": "2024-05-04T17:30:35.105027Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your score: 2616.854995644722\n",
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "# evaluation will take some time!\n",
    "sessions = evaluate(env, actor, n_games=20)\n",
    "score = sessions.mean()\n",
    "print(f\"Your score: {score}\")\n",
    "\n",
    "assert score >= 1000, \"Needs more training?\"\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-04T17:31:06.841573Z",
     "iopub.status.busy": "2024-05-04T17:31:06.840898Z",
     "iopub.status.idle": "2024-05-04T17:31:06.850679Z",
     "shell.execute_reply": "2024-05-04T17:31:06.849915Z",
     "shell.execute_reply.started": "2024-05-04T17:31:06.841538Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-16T18:43:19.559507Z",
     "start_time": "2020-09-16T18:43:19.522533Z"
    },
    "execution": {
     "iopub.execute_input": "2024-05-04T17:31:14.686216Z",
     "iopub.status.busy": "2024-05-04T17:31:14.685531Z",
     "iopub.status.idle": "2024-05-04T17:31:24.521830Z",
     "shell.execute_reply": "2024-05-04T17:31:24.520380Z",
     "shell.execute_reply.started": "2024-05-04T17:31:14.686179Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/arqa/Documents/YSDA/RL/week09_policy_II/td3_and_sac/videos/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/arqa/Documents/YSDA/RL/week09_policy_II/td3_and_sac/videos/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/arqa/Documents/YSDA/RL/week09_policy_II/td3_and_sac/videos/rl-video-episode-0.mp4\n"
     ]
    }
   ],
   "source": [
    "from gymnasium.wrappers import RecordVideo\n",
    "\n",
    "# let's hope this will work\n",
    "# don't forget to pray\n",
    "with gym.make(\"Ant-v4\", render_mode=\"rgb_array\") as env, RecordVideo(\n",
    "    env=env, video_folder=\"./videos\"\n",
    ") as env_monitor:\n",
    "    # note that t_max is 300, so collected reward will be smaller than 1000\n",
    "    evaluate(env_monitor, actor, n_games=1, t_max=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"480\" controls>\n",
       "  <source src=\"videos/rl-video-episode-0.mp4\" type=\"video/mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show video. This may not work in some setups. If it doesn't\n",
    "# work for you, you can download the videos and view them locally.\n",
    "\n",
    "from pathlib import Path\n",
    "from base64 import b64encode\n",
    "from IPython.display import HTML\n",
    "import sys\n",
    "\n",
    "video_paths = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n",
    "video_path = video_paths[-1]  # You can also try other indices\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    # https://stackoverflow.com/a/57378660/1214547\n",
    "    with video_path.open('rb') as fp:\n",
    "        mp4 = fp.read()\n",
    "    data_url = 'data:video/mp4;base64,' + b64encode(mp4).decode()\n",
    "else:\n",
    "    data_url = str(video_path)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(data_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report\n",
    "\n",
    "We'd like to collect some statistics about computational resources you spent on this task. Please, report:\n",
    "* which GPU or CPU you used: I did the work in datasphere, hence tried several settings. Turned out that the fastest one is 4vCPU) 8vCPU works slightly worse, 32vCPU almost does not work at all, and even T4 GPU works slower than 4vCPU. I guess the bottleneck is the slow pythonic, single-threaded replay buffer and the learning rate would not improve untill a more efficient implementation is provided. Anyway, this may be not an urgent issue as it learns not that bad after all.\n",
    "* number of iterations you used for training: 500k\n",
    "* wall-clock time spent (on computation =D): 1.5h"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataSphere Kernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
